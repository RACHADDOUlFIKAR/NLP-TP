{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yJMfPW8fSyR",
        "outputId": "211b1bae-deb1-4895-9b8d-42d4755e0c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates trouvées : ['12-05-1982', '03/04/1990', '23 Septembre 2010']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "texte = \"\"\"\n",
        "Paul est né le 12-05-1982 et Marie le 03/04/1990. Ils se sont rencontrés le 23 Septembre 2010.\n",
        "\"\"\"\n",
        "\n",
        "# Regex pour identifier les dates\n",
        "pattern = r'\\b(?:\\d{2}/\\d{2}/\\d{4}|\\d{2}-\\d{2}-\\d{4}|\\d{1,2} (?:Janvier|Février|Mars|Avril|Mai|Juin|Juillet|Août|Septembre|Octobre|Novembre|Décembre) \\d{4})\\b'\n",
        "\n",
        "# Recherche des dates dans le texte\n",
        "dates = re.findall(pattern, texte)\n",
        "\n",
        "print(\"Dates trouvées :\", dates)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texte = \"\"\"\n",
        "Ce dimanche 19 novembre 2023 (19-11-23) marque une date historique pour T1, qui s'impose majestueusement avec un score de 3-0 contre Weibo Gaming.\n",
        "Faker était en 06-01-09.\n",
        "\"\"\"\n",
        "\n",
        "# Regex pour identifier les dates\n",
        "pattern = r'\\b(?:\\d{1,2} (?:janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre) \\d{4}|\\d{2}-\\d{2}-\\d{4}|\\d{2}-\\d{2}-\\d{2})\\b'\n",
        "# Recherche des dates dans le texte\n",
        "# TODO\n",
        "dates = re.findall(pattern, texte)\n",
        "\n",
        "print(\"Dates trouvées :\", dates)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RoYBpFWhwo_",
        "outputId": "bb81f086-cc34-4b80-c9a8-86a2479fc3b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates trouvées : ['19 novembre 2023', '19-11-23', '06-01-09']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def eliza_chatbot(user_input):\n",
        "    # Utilisation de règles basées sur des mots-clés\n",
        "    if re.search(r'\\bsad\\b', user_input, re.IGNORECASE):\n",
        "        return \"WHY ARE YOU FEELING SAD TODAY?\"\n",
        "\n",
        "    if re.search(r'\\balways\\b', user_input, re.IGNORECASE):\n",
        "        return \"CAN YOU THINK OF A SPECIFIC EXAMPLE?\"\n",
        "\n",
        "    if re.search(r'\\blate\\b', user_input, re.IGNORECASE):\n",
        "        return \"TELL ME MORE ABOUT WHY YOUR BOYFRIEND IS ALWAYS LATE.\"\n",
        "\n",
        "    if re.search(r'\\bignore\\b', user_input, re.IGNORECASE):\n",
        "        return \"HOW DOES IT FEEL WHEN YOU FEEL IGNORED?\"\n",
        "\n",
        "    if re.search(r'\\bfather\\b', user_input, re.IGNORECASE):\n",
        "        return \"TELL ME MORE ABOUT YOUR RELATIONSHIP WITH YOUR FATHER.\"\n",
        "\n",
        "    if re.search(r'\\bhelp\\b', user_input, re.IGNORECASE):\n",
        "        return \"I'M HERE TO HELP. WHAT DO YOU NEED MORE HELP WITH?\"\n",
        "\n",
        "    # Réponse par défaut si aucune règle n'est déclenchée\n",
        "    return \"I AM NOT SURE I UNDERSTAND WHAT YOU ARE SAYING\"\n",
        "\n",
        "# Exemple d'utilisation\n",
        "user_input = input(\"You: \")\n",
        "print(\"ELIZA: \" + eliza_chatbot(user_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jve1kxK8kYQc",
        "outputId": "54007e57-521f-40a4-a4ac-b733e4fe8bb8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: \n",
            "ELIZA: I AM NOT SURE I UNDERSTAND WHAT YOU ARE SAYING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q nltk"
      ],
      "metadata": {
        "id": "jKMvqH41lz_v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import nltk\n",
        "# Données pour la Tokenisation\n",
        "nltk.download('punkt')\n",
        "# Données pour le pos_tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "I5PEvliNmBOB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs. #animals\"\n",
        "\n",
        "# Fonction pour traiter les hashtags comme un seul mot\n",
        "def custom_tokenizer(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    modified_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if tokens[i] == '#':\n",
        "            # Combine le hashtag avec le mot suivant\n",
        "            modified_tokens.append(tokens[i] + tokens[i+1])\n",
        "            i += 2  # Passe au mot suivant après le hashtag\n",
        "        else:\n",
        "            modified_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "    return modified_tokens\n",
        "\n",
        "# Tokenisation avec le tokenizer personnalisé\n",
        "tokens = custom_tokenizer(texte)\n",
        "\n",
        "# Afficher les tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPmUJGuFmRAm",
        "outputId": "33a2b225-3b81-4424-a2ab-19d8ad758ffd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', '#animals']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs. #animals\"\n",
        "\n",
        "# Utilisation de TweetTokenizer\n",
        "tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tokenizer.tokenize(texte)\n",
        "\n",
        "# Afficher les tokens\n",
        "print(tweet_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTVt59F5mk7u",
        "outputId": "7debf421-d098-463c-fee8-d5b16a2630d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', '#animals']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "\n",
        "# Tokenisation\n",
        "tokens = word_tokenize(texte)\n",
        "\n",
        "# POS-tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Afficher les POS-tags\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBeVaOnUnD76",
        "outputId": "bf13766a-ca22-4239-8e1d-6e5fe623beba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('were', 'VBD'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "\n",
        "# Initialisation du stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Tokenisation\n",
        "tokens = word_tokenize(texte)\n",
        "\n",
        "# Stemming\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Afficher les stems\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfM1vyg2oOAN",
        "outputId": "9ddaee93-099f-4050-bba1-a7ba1d097a0b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'were', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWkCyy76o-sG",
        "outputId": "d3724a05-702e-488d-cba7-090475c44070"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "\n",
        "# Initialisation du lemmatiseur\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenisation\n",
        "tokens = word_tokenize(texte)\n",
        "\n",
        "# Lemmatisation\n",
        "lemmes = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Afficher les lemmes\n",
        "print(lemmes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5cb4tNvpTCI",
        "outputId": "27d567f7-6093-443d-b356-1b6e86955d10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'were', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "\n",
        "# Initialisation du lemmatiseur\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenisation\n",
        "tokens = word_tokenize(texte)\n",
        "\n",
        "# Lemmatisation améliorée avec POS-tagging\n",
        "lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
        "\n",
        "# Afficher les lemmes\n",
        "print(lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AI8j_J5rexO",
        "outputId": "0845d895-c93d-4633-ef5b-d3dcc0dc49f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'be', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    }
  ]
}